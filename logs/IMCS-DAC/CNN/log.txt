[Tue Jan 03 15:04:35 2023|run.py|INFO] <models.CNN.Config object at 0x00000223CE776F70>
[Tue Jan 03 15:04:35 2023|run.py|INFO] Namespace(model='CNN', save_path='./datasets/IMCS-DAC/data/out.npz')
[Tue Jan 03 15:04:37 2023|modeling.py|INFO] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at C:\Users\Lenovo\.pytorch_pretrained_bert\42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f
[Tue Jan 03 15:04:37 2023|modeling.py|INFO] extracting archive file C:\Users\Lenovo\.pytorch_pretrained_bert\42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir C:\Users\Lenovo\AppData\Local\Temp\tmpzdiymgqy
[Tue Jan 03 15:04:39 2023|modeling.py|INFO] Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 21128
}

[Tue Jan 03 15:04:42 2023|modeling.py|INFO] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[Tue Jan 03 15:04:42 2023|train_eval.py|INFO] Epoch [1/3]
[Tue Jan 03 15:36:12 2023|train_eval.py|INFO] Iter:      0,  Val P: 0.006223,  Val R: 6.1725%,  Val F1: 0.01022,  Val Acc: 4.1611%,  Time: 0:31:30 *
[Tue Jan 03 16:46:29 2023|train_eval.py|INFO] Iter:    300,  Val P: 0.6832,  Val R: 50.9446%,  Val F1: 0.5583,  Val Acc: 69.3026%,  Time: 1:41:48 *
[Tue Jan 03 18:01:24 2023|train_eval.py|INFO] Iter:    600,  Val P: 0.7114,  Val R: 67.5120%,  Val F1: 0.6719,  Val Acc: 75.8260%,  Time: 2:56:43 *
[Tue Jan 03 19:13:40 2023|train_eval.py|INFO] Iter:    900,  Val P: 0.7132,  Val R: 67.2809%,  Val F1: 0.6692,  Val Acc: 77.0373%,  Time: 4:08:58 *
[Tue Jan 03 20:27:43 2023|train_eval.py|INFO] Iter:   1200,  Val P: 0.692,  Val R: 70.8882%,  Val F1: 0.6845,  Val Acc: 76.5745%,  Time: 5:23:01 
[Tue Jan 03 21:49:00 2023|train_eval.py|INFO] Iter:   1500,  Val P: 0.7167,  Val R: 70.1151%,  Val F1: 0.689,  Val Acc: 76.6107%,  Time: 6:44:18 *
[Tue Jan 03 23:13:00 2023|train_eval.py|INFO] Iter:   1800,  Val P: 0.7327,  Val R: 73.3708%,  Val F1: 0.7237,  Val Acc: 79.4841%,  Time: 8:08:18 *
[Wed Jan 04 00:24:50 2023|train_eval.py|INFO] Iter:   2100,  Val P: 0.7576,  Val R: 70.8335%,  Val F1: 0.7232,  Val Acc: 79.7255%,  Time: 9:20:08 *
[Wed Jan 04 01:34:50 2023|train_eval.py|INFO] Iter:   2400,  Val P: 0.7496,  Val R: 73.1398%,  Val F1: 0.7317,  Val Acc: 80.0475%,  Time: 10:30:08 *
[Wed Jan 04 02:44:43 2023|train_eval.py|INFO] Iter:   2700,  Val P: 0.7575,  Val R: 72.8922%,  Val F1: 0.7363,  Val Acc: 80.4821%,  Time: 11:40:01 *
[Wed Jan 04 03:01:23 2023|train_eval.py|INFO] Epoch [2/3]
[Wed Jan 04 03:55:21 2023|train_eval.py|INFO] Iter:   3000,  Val P: 0.7411,  Val R: 74.1091%,  Val F1: 0.7376,  Val Acc: 80.0918%,  Time: 12:50:39 *
[Wed Jan 04 05:05:05 2023|train_eval.py|INFO] Iter:   3300,  Val P: 0.7237,  Val R: 74.8528%,  Val F1: 0.7259,  Val Acc: 79.4237%,  Time: 14:00:23 
[Wed Jan 04 06:14:20 2023|train_eval.py|INFO] Iter:   3600,  Val P: 0.7416,  Val R: 73.0942%,  Val F1: 0.7274,  Val Acc: 80.0394%,  Time: 15:09:38 
[Wed Jan 04 07:23:37 2023|train_eval.py|INFO] Iter:   3900,  Val P: 0.7307,  Val R: 75.1155%,  Val F1: 0.7365,  Val Acc: 80.3855%,  Time: 16:18:55 
[Wed Jan 04 07:36:12 2023|train_eval.py|INFO] No optimization for a long time, auto-stopping...
