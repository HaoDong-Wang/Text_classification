[Fri Jan 13 13:35:24 2023|run.py|INFO] <models.RCNN.Config object at 0x000002209DB39F70>
[Fri Jan 13 13:35:24 2023|run.py|INFO] Namespace(model='RCNN', save_path='./datasets/CHIP-CTC/data/out.npz')
[Fri Jan 13 13:35:25 2023|modeling.py|INFO] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at C:\Users\Lenovo\.pytorch_pretrained_bert\42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f
[Fri Jan 13 13:35:25 2023|modeling.py|INFO] extracting archive file C:\Users\Lenovo\.pytorch_pretrained_bert\42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir C:\Users\Lenovo\AppData\Local\Temp\tmp8hlrcws2
[Fri Jan 13 13:35:28 2023|modeling.py|INFO] Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 21128
}

[Fri Jan 13 13:35:31 2023|modeling.py|INFO] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[Fri Jan 13 13:35:31 2023|train_eval.py|INFO] Epoch [1/3]
[Fri Jan 13 13:47:17 2023|train_eval.py|INFO] Iter:      0,  Val P: 0.0008778,  Val R: 1.5617%,  Val F1: 0.001662,  Val Acc: 3.3455%,  Time: 0:11:46 *
[Fri Jan 13 15:33:56 2023|train_eval.py|INFO] Epoch [2/3]
[Fri Jan 13 16:53:29 2023|train_eval.py|INFO] Iter:    300,  Val P: 0.7435,  Val R: 67.6287%,  Val F1: 0.677,  Val Acc: 82.0880%,  Time: 3:17:58 *
[Fri Jan 13 17:28:23 2023|train_eval.py|INFO] Epoch [3/3]
