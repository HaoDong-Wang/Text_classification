[Fri Dec 16 17:03:13 2022|run.py|INFO] <models.bert.Config object at 0x0000016F21822A60>
[Fri Dec 16 17:03:14 2022|modeling.py|INFO] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at C:\Users\Lenovo\.pytorch_pretrained_bert\42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f
[Fri Dec 16 17:03:14 2022|modeling.py|INFO] extracting archive file C:\Users\Lenovo\.pytorch_pretrained_bert\42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir C:\Users\Lenovo\AppData\Local\Temp\tmpjnt3bs2e
[Fri Dec 16 17:03:17 2022|modeling.py|INFO] Model config {
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 21128
}

[Fri Dec 16 17:03:18 2022|train_eval.py|INFO] Epoch [1/3]
[Fri Dec 16 17:17:09 2022|train_eval.py|INFO] Iter:      0,  Val P: 0.01166,  Val R: 3.7786%,  Val F1: 0.008811,  Val Acc: 4.4650%,  Time: 0:13:51 *
[Fri Dec 16 18:20:57 2022|train_eval.py|INFO] Iter:    300,  Val P: 0.4286,  Val R: 37.5376%,  Val F1: 0.3733,  Val Acc: 74.7331%,  Time: 1:17:39 *
[Fri Dec 16 19:24:41 2022|train_eval.py|INFO] Iter:    600,  Val P: 0.6232,  Val R: 52.0723%,  Val F1: 0.5355,  Val Acc: 79.5236%,  Time: 2:21:23 *
[Fri Dec 16 20:30:36 2022|train_eval.py|INFO] Iter:    900,  Val P: 0.6421,  Val R: 59.2164%,  Val F1: 0.5976,  Val Acc: 81.9058%,  Time: 3:27:18 *
[Fri Dec 16 20:40:30 2022|train_eval.py|INFO] Epoch [2/3]
[Fri Dec 16 21:40:43 2022|train_eval.py|INFO] Iter:   1200,  Val P: 0.7103,  Val R: 65.5869%,  Val F1: 0.6654,  Val Acc: 83.7672%,  Time: 4:37:26 *
[Fri Dec 16 22:48:26 2022|train_eval.py|INFO] Iter:   1500,  Val P: 0.7418,  Val R: 67.9152%,  Val F1: 0.677,  Val Acc: 83.0513%,  Time: 5:45:08
[Fri Dec 16 23:48:34 2022|train_eval.py|INFO] Iter:   1800,  Val P: 0.7805,  Val R: 70.0538%,  Val F1: 0.7163,  Val Acc: 84.2879%,  Time: 6:45:16 *
[Sat Dec 17 00:06:18 2022|train_eval.py|INFO] Epoch [3/3]
[Sat Dec 17 00:48:16 2022|train_eval.py|INFO] Iter:   2100,  Val P: 0.7907,  Val R: 76.5625%,  Val F1: 0.7649,  Val Acc: 84.9388%,  Time: 7:44:59 *
[Sat Dec 17 01:48:38 2022|train_eval.py|INFO] Iter:   2400,  Val P: 0.8139,  Val R: 79.4954%,  Val F1: 0.7961,  Val Acc: 85.3033%,  Time: 8:45:20 *
[Sat Dec 17 02:49:32 2022|train_eval.py|INFO] Iter:   2700,  Val P: 0.8411,  Val R: 78.0644%,  Val F1: 0.7963,  Val Acc: 85.1211%,  Time: 9:46:15 *
